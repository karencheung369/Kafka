{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Kafka Setup Guide",
      "provenance": [],
      "authorship_tag": "ABX9TyOdApcAClwChOjxzGSZ6xwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karencheung369/Kafka/blob/main/Apache_Kafka_Setup_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Apache Kafka</b></h1></center>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p><b>\n",
        "Not only is Apache Kafka is a distributed streaming platform, but it is also a messaging system, as well as a storage system\n",
        "<br><br>\n",
        "A streaming platform with three key capabilities:<br>\n",
        "*  Publish and subscribe to streams of records like a message queue or enterprise messaging system.<br>\n",
        "*  Store streams of records in a fault-tolerant durable way.<br>\n",
        "*    Process streams of records as they occur.<br>\n",
        "<br>\n",
        "Generally used for two broad classes of applications: <br>\n",
        "*   Building reliable real-time streaming data pipelines to get data between systems or applications.<br>\n",
        "*   Building real-time streaming applications that transform or react to the streams of data.\n",
        "</b></p>\n",
        "<br>\n",
        "<center><h1><b>Kafka Architecture</b><h1></center>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p><b>\n",
        "1. Kafka stores key-value messages or records that come from Producers which are processes. <br>\n",
        "2. data can be partitioned into different partitions within different \"topics\". <br>\n",
        "3. Within a partition, messages are ordered by their offsets  (position of a message within a partition); indexed and stored with a timestamp. <br>\n",
        "4. Consumers are processes that read messages from partitions.\n",
        "</b>"
      ],
      "metadata": {
        "id": "5LSbF3O42IU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1XnyUzQ_I0fDSbljTXNmQjOPafrdGDgJW/preview\" width=\"440\" height=\"330\" allow=\"autoplay\"></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "3LjTbv6kDkwd",
        "outputId": "7f57076a-42c9-4db6-a85c-e5c194f02b2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://drive.google.com/file/d/1XnyUzQ_I0fDSbljTXNmQjOPafrdGDgJW/preview\" width=\"440\" height=\"330\" allow=\"autoplay\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Producer(s)</b> writes messages into <b>topic(s).</b><br>\n",
        "<b>Topic</b> has 1 or multiple <b>partitions.</b><br>\n",
        "<b>Messages</b> in a <b>topic</b> across <b>partitions</b> are not guaranteed their order.<br>\n",
        "<b>Offset</b> is the position of a message within a partition.<br>\n",
        "<b>Partition</b> is like a Message Queue; messages are ordered.<br>\n",
        "<b>Cnsumer(s)</b> can subscribe to <b>topic(s)</b> and read message, a <b>topic can also be subscribed by multiple consumers.\n",
        "\n",
        "\n",
        "Kafka runs on a cluster of one or more servers (called brokers), and the partitions of all topics are distributed across the cluster nodes (i.e. brokers). Additionally, partitions are replicated to multiple brokers. This architecture allows Kafka to deliver massive streams of messages in a fault-tolerant fashion"
      ],
      "metadata": {
        "id": "5vezB3cwGwYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/10mucKK_3yCdUywFv6i3g_EXBbFh7OWgj/preview\" width=\"580\" height=\"350\" allow=\"autoplay\"></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "R90v18jyDj_M",
        "outputId": "3e3d0dff-027b-4124-f8bf-2f95cf9e9fc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://drive.google.com/file/d/10mucKK_3yCdUywFv6i3g_EXBbFh7OWgj/preview\" width=\"580\" height=\"350\" allow=\"autoplay\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Build Kafka cluster</b></h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<p>\n",
        "This environment has Hadoop and Spark running on the 11 slave nodes. I will build Kafka cluster on 8 nodes (broker 0 to 7).<br>\n",
        "</p>"
      ],
      "metadata": {
        "id": "IQhe-mNALaSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1ba6KN3HBfHnyys8W02LHBMsXvTOG82-d/preview\" width=\"850\" height=\"330\" allow=\"autoplay\"></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "7K2xbAYkX8-m",
        "outputId": "d5e10543-630d-40b6-d834-9d8d3a6b5c9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://drive.google.com/file/d/1ba6KN3HBfHnyys8W02LHBMsXvTOG82-d/preview\" width=\"850\" height=\"330\" allow=\"autoplay\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Kafka cluster and the Hadoop & Spark cluster are logically separate and independent, but since Kafka cluster physically overlaps with the Hadoop & Spark cluster on those 8 container nodes which have 7G memory, so they may compete for the hardware resources.</p>"
      ],
      "metadata": {
        "id": "WkPxRcNgW1Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://drive.google.com/file/d/1SC0npK6VQodidjsOPj9DDB2-zkNbfacH/preview\" width=\"700\" height=\"200\" allow=\"autoplay\"></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "wcxuzWgbLZyH",
        "outputId": "a28f430c-a623-419a-9135-65cc96d66655"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://drive.google.com/file/d/1SC0npK6VQodidjsOPj9DDB2-zkNbfacH/preview\" width=\"700\" height=\"200\" allow=\"autoplay\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Download Kafka</b></h1></center>\n",
        "<p>Install Kafka on one node (broker 0) first, then later copy it to other 7 nodes.</p>\n",
        "\n",
        "```\n",
        "cd /opt\n",
        "sudo wget https://archive.apache.org/dist/kafka/2.4.0/kafka_2.11-2.4.0.tgz\n",
        "```\n",
        "<center><h1><b>Configure Kafka Profile</b></h1></center>\n",
        "<p>Add the paths to /etc/profile</p>\n",
        "\n",
        "\n",
        "```\n",
        " sudo vim /etc/profile\n",
        " export KAFKA_HOME=/opt/kafka_2.11-2.4.0\n",
        " export CLASSPATH=$CLASSPATH:$KAFKA_HOME/libs\n",
        " export PATH=$PATH:$KAFKA_HOME/bin\n",
        "```\n",
        "Source the profile, also do this whenever you see “bash: xxx: command not found”.\n",
        "\n",
        "```\n",
        "source /etc/profile\n",
        "```\n",
        "Create the log directory and the data directory of zookeeper.\n",
        "\n",
        "\n",
        "```\n",
        "cd /opt/kafka_2.11-2.4.0 mkdir zookeeper\n",
        "mkdir log\n",
        "mkdir log/zookeeper mkdir log/kafka\n",
        "```\n",
        "Create the log directory and the data directory of zookeeper\n",
        "```\n",
        "cd /opt/kafka_2.11-2.4.0 mkdir zookeeper\n",
        "mkdir log\n",
        "mkdir log/zookeeper mkdir log/kafka\n",
        "```\n"
      ],
      "metadata": {
        "id": "yczvfZhgd79_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Configure zookeeper</b></h1></center>\n",
        "\n",
        "Configure the Zookeeper environment by editing zookeeper.properties.<br>\n",
        "Role of ZooKepper in Kafka:<br>\n",
        "*   Store nodes and topic registries\n",
        "*   Check if the Kafka Broker is alive\n",
        "*   Node failure: elect the new leader\n",
        "*   ZooKeeper runs as a JVM\n",
        "```\n",
        "cd /opt/kafka_2.11-2.4.0\n",
        "vim config/zookeeper.properties\n",
        "```\n",
        "Change the “dataDir” and “dataLogDir”.<br>\n",
        "dataDir is the location where ZooKeeper will store the in-memory database snapshots.<br>\n",
        "dataLogDir option will direct the machine to write the transaction log to the dataLogDir rather than the dataDir.\n",
        "\n",
        "```\n",
        "dataDir=/opt/kafka_2.11-2.4.0/zookeeper\n",
        "dataLogDir=/opt/kafka_2.11-2.4.0/log/zookeeper\n",
        "```\n",
        "Add the following lines in zookeeper.properties.<br>\n",
        "A tickTime of 2000 milliseconds is the suggested interval between heartbeats.\n",
        "\n",
        "```\n",
        " #About connection\n",
        " tickTime=2000\n",
        " initLimit=10\n",
        " syncLimit=5\n",
        "```\n",
        "Configure the Zookeeper by editing zookeeper.properties.<br>\n",
        "Add all the 8 nodes in zookeeper.properties.<br>\n",
        "Make sure each broker has a unique broker.id (0, 1, 2, ... 7)\n",
        "```\n",
        " server.0=hostnamesX-x1:2888:3888\n",
        " server.1=hostnamesX-x2:2888:3888\n",
        " server.2=hostnamesY-x1:2888:3888\n",
        " server.3=hostnamesY-x2:2888:3888\n",
        " server.4=hostnamesZ-x1:2888:3888\n",
        " server.5=hostnamesZ-x2:2888:3888\n",
        " server.6=hostnamesK-x1:2888:3888\n",
        " server.7=hostnamesK-x2:2888:3888\n",
        "```\n",
        "Change the “maxClientCnxns” to 60<br>\n",
        "Maximum number of client connections for a ZooKeeper server is Default: 60.<br>If set this to 0, it is unlimited.\n",
        "```\n",
        "maxClientCnxns=60\n",
        "```\n",
        "Each node running Zookeeper contains ID in myid file stored in the dataDir folder.<br>\n",
        "Assign a unique ID for each Broker at each node to be managed by ZooKeeper:<br>\n",
        "*   Create the “myid” file in the “dataDir” directory “/opt/kafka_2.11-2.4.0/zookeeper”\n",
        "```\n",
        " cd /opt/kafka_2.11-2.4.0/zookeeper\n",
        " echo 0 > myid\n",
        "```\n",
        "*   View the content stored in myid file using cat command.\n",
        "```\n",
        " cat myid\n",
        "```\n",
        "Each node in the ZooKeeper cluster has to be assigned with a unique integer identifier used by zookeeper to\n",
        "recognize different nodes which run Zookeeper services.\n",
        "\n"
      ],
      "metadata": {
        "id": "KanIRmBhwVNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Configure Kafka Server</b></h1></center>\n",
        "<p>\n",
        "Configure the Kafka server environment by editing server.properties.\n",
        "\n",
        "```\n",
        "cd /opt/kafka_2.11-2.4.0\n",
        "vim config/server.properties\n",
        "```\n",
        "broker.id should be different on different nodes. For simplicity can set broker.id to be same as that in “myid” file of current node.\n",
        "\n",
        "```\n",
        "broker.id=0\n",
        "```\n",
        "Delete the “#” next to listeners and add the hostname of current node.\n",
        "\n",
        "```\n",
        "listeners=PLAINTEXT://hostnameX-x1:9092\n",
        "```\n",
        "Set the log.dirs.\n",
        "```\n",
        "log.dirs=/opt/kafka_2.11-2.4.0/log/kafka\n",
        "```\n",
        "Set the default number of partitions of a topic.<br>\n",
        "num.partitions sets the default number of partitions of a topic.\n",
        "```\n",
        "num.partitions=3\n",
        "```\n",
        "Change the zookeeper.connect.\n",
        "```\n",
        " zookeeper.connect=studentX-x1:2181,studentX-x2:2181,studentY-\n",
        "x1:2181,studentY-x2:2181,studentZ-x1:2181,studentZ- x2:2181,studentK-x1:2181,studentK-x2:2181\n",
        "```\n",
        "Then Change “zookeeper.connection.timeout.ms” from 6000 to 6000000.\n",
        "```\n",
        " zookeeper.connection.timeout.ms=6000000\n",
        "```\n",
        "Add the following line in server.properties.\n",
        "```\n",
        "delete.topic.enable=true\n",
        "```\n",
        "Now copy Kafka and its configuration files set above on the first node to the rest of the other 7 nodes.\n",
        "</p>\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "jLE0b-7GwVD7"
      }
    }
  ]
}